{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions on test dataset\n",
    "\n",
    "This notebook was created to use trained and hiperparametrized models to predict which passengers has survived on the test set. The first thing we gotta do is to load the libraries and functions we have created to pre process test data and make models predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#pre process\n",
    "from modelling.pre_process import pre_processing\n",
    "\n",
    "# modelling\n",
    "from modelling.fit import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing test dataset\n",
    "\n",
    "As you can see there are some variables in the dataset that contains NaN values. This is problematic because Sklearn models can't handle with missing values. Lets fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId: 0\n",
      "Pclass: 0\n",
      "Name: 0\n",
      "Sex: 0\n",
      "Age: 86\n",
      "SibSp: 0\n",
      "Parch: 0\n",
      "Ticket: 0\n",
      "Fare: 1\n",
      "Cabin: 327\n",
      "Embarked: 0\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "to_predict = pd.read_csv('test.csv')\n",
    "\n",
    "# Saving Ids informations \n",
    "to_predict.index = to_predict['PassengerId']\n",
    "\n",
    "#checking how many missing values are in each variable\n",
    "for c in to_predict.columns:\n",
    "    print(f'{c}: {to_predict[c].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values on Age, Fare and Cabin variables. First, lets complete the only one missing value on Fare variable with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict['Fare'].fillna(to_predict['Fare'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Lets, check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId: 0\n",
      "Pclass: 0\n",
      "Name: 0\n",
      "Sex: 0\n",
      "Age: 86\n",
      "SibSp: 0\n",
      "Parch: 0\n",
      "Ticket: 0\n",
      "Fare: 0\n",
      "Cabin: 327\n",
      "Embarked: 0\n"
     ]
    }
   ],
   "source": [
    "for c in to_predict.columns:\n",
    "    print(f'{c}: {to_predict[c].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to worry about the missing values on Cabin variable because the entire column will be dropped as well the variables PassengerId, Name, Ticket and Embarked. \n",
    "\n",
    "Therefore, we gotta work on the missing values on Age. This will not be a problem since we have build a method to deal with that! The method created will fill NaN values with an OLS regression!\n",
    "\n",
    "Now we are ready to do all the pre processing steps. Lets do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting and removing outliers...\n",
      "Total rows deleted containing outliers: 99\n"
     ]
    }
   ],
   "source": [
    "df = pre_processing(to_predict) #create the class of pre processing\n",
    "df.select_columns(drop=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']) #drop unecessary columns\n",
    "df.Create_I_Matrix() #create the design matrix (One Hot Encoding)\n",
    "df.Standardize() # Standartize numerical columns\n",
    "# df.FactorCategorical()\n",
    "# df.RemoveOutliers() # Remove outliers\n",
    "df.fill_nan_ols('Age') # Fill NaN's values with regression on Age column\n",
    "df.df.drop('Parch_0', axis=1, inplace=True) # Drop an extra column of Parch category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "At this point our test dataset is ready to be used to predict the survivers. To do that let's use some models that we already hiperparemetrized and trained with the train dataset. The classifiers trained were the Logistic model, Naive Bayes model, Random Forest model, Multilayer Perceptron, XGboost and Support Vector Machine model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Naive_Bayes  SVC  XGBoost  Random_Forest  MLP  Logistic  PassengerId\n",
       "0            0    0        0              0    0         0          892\n",
       "1            1    0        0              0    0         0          893\n",
       "2            0    0        0              0    0         0          894\n",
       "3            0    0        1              0    0         0          895\n",
       "4            1    1        1              1    1         1          896"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choosing trained models to do predictions\n",
    "models = ['Logistic', \n",
    "          'Naive_Bayes', \n",
    "          'Random_Forest', \n",
    "          'MLP',\n",
    "          'XGBoost',\n",
    "          'SVC']\n",
    "\n",
    "# Creating the class of predictions\n",
    "prediction = predict_from_load_model(X_test=df.df, \n",
    "                                     choosen_models=models)\n",
    "\n",
    "# Making predictions!\n",
    "y_hats = prediction.do_it()\n",
    "y_hats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen on the comparison models notebook, the best model according to the $F_\\beta$ score was the Random Forest classifier. Thus, we gotta to send the predictions of this model to kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = y_hats[['PassengerId','Random_Forest']]\n",
    "predictions.columns = ['PassengerId','Survived']\n",
    "predictions.to_csv('Survived_predictons.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
